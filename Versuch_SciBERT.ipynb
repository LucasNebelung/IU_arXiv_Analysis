{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "### Setting up Modules and loading the whole data frame\n",
    "###################################\n",
    "\n",
    "\n",
    "### import XPU for my local Intel Laptop \n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "### import modules and model \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select XPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"xpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# specify model and tokenizer\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# import database \n",
    "df = pd.read_pickle (\"df_clean.pkl\")\n",
    "\n",
    "# define consistent color mapping\n",
    "color_mapping = {\n",
    "    \"Physics\": \"Purple\",\n",
    "    \"Computer Science\": \"orange\",\n",
    "    \"Mathematics\": \"blue\",\n",
    "    \"Statistics\": \"red\",\n",
    "    \"Quantitative Biology\": \"green\",\n",
    "    \"Quantitative Finance\": \"brown\",\n",
    "    \"Other\": \"gray\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define consistent color mapping\n",
    "color_mapping = {\n",
    "    \"Physics\": \"Purple\",\n",
    "    \"Computer Science\": \"orange\",\n",
    "    \"Mathematics\": \"blue\",\n",
    "    \"Statistics\": \"red\",\n",
    "    \"Quantitative Biology\": \"green\",\n",
    "    \"Quantitative Finance\": \"brown\",\n",
    "    \"Other\": \"gray\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "####### Selecting the Subset to be analysed\n",
    "\n",
    "subset_df = df.sample(n=10000, random_state=42).reset_index(drop=True)    #[(df[\"year\"] >= 1990) & (df[\"year\"] <= 2024)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "###### create Word Embeddings in Latent Space\n",
    "\n",
    "\n",
    "# Define a helper function to extract the [CLS] embedding for a given text\n",
    "def get_cls_embedding(text):\n",
    "    # Tokenize the text with a maximum length of 512 tokens (truncation applied)\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    # Move inputs to the device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extract the [CLS] token embedding (first token in the sequence)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: (1, hidden_size)\n",
    "    # Remove the batch dimension and convert to numpy array\n",
    "    return cls_embedding.squeeze(0).cpu().numpy()\n",
    "\n",
    "# experimenting with a few Scaling Methods \n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "# embeddings = scaler.fit_transform(raw_embeddings)\n",
    "\n",
    "# Generate embeddings for each abstract in the sample\n",
    "embeddings =[get_cls_embedding(abstract) for abstract in subset_df[\"abstract\"]]\n",
    "embeddings = np.array(embeddings)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "####### Run a PCA and estimate Explained Variance incl. Scree Plot\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Run full PCA on the embeddings ()\n",
    "pca= PCA().fit(embeddings)\n",
    "\n",
    "# define explained and create enumarated array \n",
    "explained_variance = pca.explained_variance_ratio_ * 100  # convert to percentages\n",
    "components = np.arange(1, len(explained_variance) + 1)\n",
    "\n",
    "# Filter to include only components with >= 1% explained variance\n",
    "mask = explained_variance >= 1\n",
    "filtered_components = components[mask]\n",
    "filtered_explained_variance = explained_variance[mask]\n",
    "\n",
    "# Plot the scree plot using only filtered components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_components, filtered_explained_variance, color='skyblue')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.title('Scree Plot (Components with ≥ 1% Explained Variance)')\n",
    "plt.xticks(filtered_components)\n",
    "plt.ylim(0,100)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "########## create cumulatative scree plot \n",
    "cumulative_varianve_explained = np.cumsum(filtered_explained_variance)\n",
    "\n",
    "#plot cumulative variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_components,cumulative_varianve_explained, color='skyblue')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Cumulatative Explained Variance (%)')\n",
    "plt.title('Plot (Components with ≥ 1% Explained Variance)')\n",
    "plt.xticks(filtered_components)\n",
    "plt.ylim(0,100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "###### plot PCA with colors \n",
    "\n",
    "pca_projection = pca.transform(embeddings)\n",
    "\n",
    "\n",
    "# Get unique fields for the plot\n",
    "unique_fields = subset_df[\"field\"].unique()\n",
    "# define year bins\n",
    "year_bins = [(2000, 2001), (2002, 2003), (2004, 2005), (2023, 2024), (2025, 2025)]\n",
    "\n",
    "for start,end in year_bins: \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # create mask_years to be able to iterate through each year\n",
    "    mask_years = (subset_df[\"year\"] >= start) & (subset_df[\"year\"] <= end)\n",
    "    # creating year bin\n",
    "    df_binned_years = subset_df[mask_years]\n",
    "    print(f\"Number of publications from {start} to {end}: {len(df_binned_years)}\")\n",
    "\n",
    "    # Plot each field with its specific color from the color mapping\n",
    "    for field in unique_fields:\n",
    "        if field in color_mapping:  # Check if the field is in our color mapping\n",
    "            # Create a combined mask for both year range and field\n",
    "            combined_mask = mask_years & (subset_df[\"field\"] == field)\n",
    "            # Only plot if there are any points matching the criteria\n",
    "            if combined_mask.any():\n",
    "                plt.scatter(\n",
    "                    pca_projection[combined_mask, 0],  # PC1\n",
    "                    pca_projection[combined_mask, 1],  # PC2\n",
    "                    label=f\"{field} ({start}-{end})\",  # Add year range to label\n",
    "                    color=color_mapping[field],\n",
    "                    alpha=0.7,\n",
    "                    s=80  # Point size\n",
    "                )\n",
    "\n",
    "    plt.title(\"PCA of SciBERT Embeddings\", fontsize=15)\n",
    "    plt.xlabel(\"Principal Component 1\", fontsize=12)\n",
    "    plt.ylabel(\"Principal Component 2\", fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print the explained variance for the first two components\n",
    "pc1_variance = explained_variance[0]\n",
    "pc2_variance = explained_variance[1]\n",
    "print(f\"PC1 explains {pc1_variance:.2f}% of the variance\")\n",
    "print(f\"PC2 explains {pc2_variance:.2f}% of the variance\")\n",
    "print(f\"Together they explain {pc1_variance + pc2_variance:.2f}% of the variance\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "### creating embedded vectors of official subcategories  as a reference using SciBert\n",
    "\n",
    "# create dataframe from pickle file\n",
    "df_cats_map = pd.read_pickle(\"df_cats_map.pkl\")\n",
    "\n",
    "embedding_sources = []  #Only used to verify if else statement is working properly, could be removed\n",
    "\n",
    "# Generate embeddings for each category, using long_description if available, otherwise short_description\n",
    "embeddings_cat = []\n",
    "for _, row in df_cats_map.iterrows():\n",
    "    # Use long_description if it is not just a placeholder text\n",
    "    if  \"Description coming soon\" not in row['long_description']:\n",
    "        embedding_sources.append(\"long\")\n",
    "        embeddings_cat.append(get_cls_embedding(row['long_description']))\n",
    "    else:\n",
    "        embedding_sources.append(\"short\")\n",
    "        embeddings_cat.append(get_cls_embedding(row['short_description']))\n",
    "\n",
    "embeddings_cat = np.array(embeddings_cat)\n",
    "print(\"Embeddings shape:\", embeddings_cat.shape)\n",
    "\n",
    "# Create a summary of which descriptions were used\n",
    "source_counts = pd.Series(embedding_sources).value_counts()\n",
    "print(\"Count of descriptions used:\")\n",
    "print(source_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#### plotting embeddings_cat in the same PCA and same way \n",
    "\n",
    "pca_projection_cats = pca.transform(embeddings_cat)\n",
    "\n",
    "# Create a figure for plotting category embeddings\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for field in unique_fields:\n",
    "    if field in color_mapping and not pd.isna(field):  # Check if the field is in our color mapping and not NaN\n",
    "        # Create a mask for the current field in df_cats_map\n",
    "        mask_cats = (df_cats_map[\"field\"] == field)\n",
    "        # Only plot if there are any points matching the criteria\n",
    "        if mask_cats.any():\n",
    "            plt.scatter(\n",
    "                pca_projection_cats[mask_cats, 0],  # PC1\n",
    "                pca_projection_cats[mask_cats, 1],  # PC2\n",
    "                label=field,  # Simply use the field name\n",
    "                color=color_mapping[field],\n",
    "                alpha=0.7,\n",
    "                s=80  # Point size\n",
    "            )\n",
    "\n",
    "plt.title(\"PCA of SciBERT Category Embeddings\", fontsize=15)\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=12)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### MDS Attempt\n",
    "'''\n",
    "from sklearn.manifold import MDS\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import manifold\n",
    "\n",
    "mds = MDS (n_components=2, random_state=0)\n",
    "mds_projection = mds.fit_transform(embeddings)\n",
    "\n",
    "# Plot the MDS projection\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get unique fields for the plot\n",
    "unique_fields = subset_df[\"field\"].unique()\n",
    "\n",
    "# Plot each field with its specific color from the color mapping\n",
    "for field in unique_fields:\n",
    "    if field in color_mapping:  # Check if the field is in our color mapping\n",
    "        mask = subset_df[\"field\"] == field\n",
    "        plt.scatter(\n",
    "            mds_projection[mask, 0],  # MDS dimension 1\n",
    "            mds_projection[mask, 1],  # MDS dimension 2\n",
    "            label=field,\n",
    "            color=color_mapping[field],\n",
    "            alpha=0.7,\n",
    "            s=80  # Point size\n",
    "        )\n",
    "\n",
    "plt.title(\"MDS of SciBERT Embeddings\", fontsize=15)\n",
    "plt.xlabel(\"MDS Dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"MDS Dimension 2\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#### creating 3D PCA Plot\n",
    "\n",
    "'''\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume embeddings is your NumPy array of shape (num_samples, 768)\n",
    "pca_3d = PCA(n_components=3)\n",
    "embeddings_3d = pca_3d.fit_transform(embeddings)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Example: color-code by field if you have that column in your DataFrame\n",
    "# Create a color mapping (this is just an example; adjust as needed)\n",
    "unique_fields = subset_df[\"field\"].unique()\n",
    "cmap = plt.cm.get_cmap(\"tab10\", len(unique_fields))\n",
    "field_to_color = {field: cmap(i) for i, field in enumerate(unique_fields)}\n",
    "\n",
    "for field in unique_fields:\n",
    "    idx = subset_df[\"field\"] == field\n",
    "    ax.scatter(\n",
    "        embeddings_3d[idx, 0],\n",
    "        embeddings_3d[idx, 1],\n",
    "        embeddings_3d[idx, 2],\n",
    "        label=field,\n",
    "        color=field_to_color[field],\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax.set_title(\"3D PCA of SciBERT Abstract Embeddings\")\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "ax.legend()\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#### creating 3D interactive PCA Plot\n",
    "\n",
    "'''\n",
    "import plotly.express as px\n",
    "import nbformat \n",
    "\n",
    "# Suppose 'embeddings' is your NumPy array of SciBERT embeddings (num_samples x 768)\n",
    "# Run PCA to reduce to 3 dimensions:\n",
    "pca_3d = PCA(n_components=3)\n",
    "embeddings_3d = pca_3d.fit_transform(embeddings)\n",
    "\n",
    "# Create a DataFrame with the PCA components and your grouping variable ('field')\n",
    "df_pca = pd.DataFrame(embeddings_3d, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "df_pca[\"field\"] = subset_df[\"field\"].values  # Ensure the order matches your embeddings\n",
    "\n",
    "# Create the interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_pca, x=\"PC1\", y=\"PC2\", z=\"PC3\",\n",
    "    color=\"field\",                # Color-code by field\n",
    "    title=\"Interactive 3D PCA of SciBERT Abstract Embeddings\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##### Deciding Whether to use \"abstract\" or \"title\" column to create embedded vector\n",
    "##### -> Question to be answered is: Do the Abstracts exceed the max. input length of 512tokens ?\n",
    "\n",
    "'''\n",
    "# Computing the tokenized length of each abstract.\n",
    "df_csLG[\"abstract_token_length\"] = df_csLG[\"abstract\"].apply(\n",
    "    lambda x: len(tokenizer.encode(x, add_special_tokens=True))\n",
    ")\n",
    "\n",
    "# Checking the maximum, mean, and distribution.\n",
    "max_length = df_csLG[\"abstract_token_length\"].max()\n",
    "mean_length = df_csLG[\"abstract_token_length\"].mean()\n",
    "description = df_csLG[\"abstract_token_length\"].describe()\n",
    "\n",
    "print(\"Max token length:\", max_length)\n",
    "print(\"Mean token length:\", mean_length)\n",
    "print(description)\n",
    "\n",
    "#### Estimating number of rows above 512\n",
    "# Filter rows where token length exceeds 512 from cs.LG abstracts\n",
    "df_long_abstracts = df_csLG[df_csLG[\"abstract_token_length\"] > 512]\n",
    "\n",
    "# How many such rows?\n",
    "count_long = len(df_long_abstracts)\n",
    "\n",
    "print(f\"Number of abstracts above 512 tokens: {count_long}\")\n",
    "\n",
    "\n",
    "##############      Console Output: Number of abstracts above 512 tokens: 61   (i.e. 61/100000)\n",
    "##############      -> Therefore decision to use Abstract as opposed to titles \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
